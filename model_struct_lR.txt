backbone.layer2.0.conv1.weight: 32768
backbone.layer2.0.conv2.weight: 147456
backbone.layer2.0.conv3.weight: 65536
backbone.layer2.0.downsample.0.weight: 131072
backbone.layer2.1.conv1.weight: 65536
backbone.layer2.1.conv2.weight: 147456
backbone.layer2.1.conv3.weight: 65536
backbone.layer2.2.conv1.weight: 65536
backbone.layer2.2.conv2.weight: 147456
backbone.layer2.2.conv3.weight: 65536
backbone.layer2.3.conv1.weight: 65536
backbone.layer2.3.conv2.weight: 147456
backbone.layer2.3.conv3.weight: 65536
backbone.layer3.0.conv1.weight: 131072
backbone.layer3.0.conv2.weight: 589824
backbone.layer3.0.conv3.weight: 262144
backbone.layer3.0.downsample.0.weight: 524288
backbone.layer3.1.conv1.weight: 262144
backbone.layer3.1.conv2.weight: 589824
backbone.layer3.1.conv3.weight: 262144
backbone.layer3.2.conv1.weight: 262144
backbone.layer3.2.conv2.weight: 589824
backbone.layer3.2.conv3.weight: 262144
backbone.layer3.3.conv1.weight: 262144
backbone.layer3.3.conv2.weight: 589824
backbone.layer3.3.conv3.weight: 262144
backbone.layer3.4.conv1.weight: 262144
backbone.layer3.4.conv2.weight: 589824
backbone.layer3.4.conv3.weight: 262144
backbone.layer3.5.conv1.weight: 262144
backbone.layer3.5.conv2.weight: 589824
backbone.layer3.5.conv3.weight: 262144
backbone.layer4.0.conv1.weight: 524288
backbone.layer4.0.conv2.weight: 2359296
backbone.layer4.0.conv3.weight: 1048576
backbone.layer4.0.downsample.0.weight: 2097152
backbone.layer4.1.conv1.weight: 1048576
backbone.layer4.1.conv2.weight: 2359296
backbone.layer4.1.conv3.weight: 1048576
backbone.layer4.2.conv1.weight: 1048576
backbone.layer4.2.conv2.weight: 2359296
backbone.layer4.2.conv3.weight: 1048576
neck.convs.0.0.weight: 131072
neck.convs.0.1.weight: 256
neck.convs.0.1.bias: 256
neck.convs.1.0.weight: 262144
neck.convs.1.1.weight: 256
neck.convs.1.1.bias: 256
neck.convs.2.0.weight: 524288
neck.convs.2.1.weight: 256
neck.convs.2.1.bias: 256
neck.convs.3.0.weight: 4718592
neck.convs.3.1.weight: 256
neck.convs.3.1.bias: 256
transformer.level_embeds: 1024
transformer.alpha: 3
transformer.enc_output.weight: 65536
transformer.enc_output.bias: 256
transformer.enc_output_norm.weight: 256
transformer.enc_output_norm.bias: 256
transformer.encoder.layers.0.pre_attention.in_proj_weight: 196608
transformer.encoder.layers.0.pre_attention.in_proj_bias: 768
transformer.encoder.layers.0.pre_attention.out_proj.weight: 65536
transformer.encoder.layers.0.pre_attention.out_proj.bias: 256
transformer.encoder.layers.0.pre_norm.weight: 256
transformer.encoder.layers.0.pre_norm.bias: 256
transformer.encoder.layers.0.self_attn.sampling_offsets.weight: 65536
transformer.encoder.layers.0.self_attn.sampling_offsets.bias: 256
transformer.encoder.layers.0.self_attn.attention_weights.weight: 32768
transformer.encoder.layers.0.self_attn.attention_weights.bias: 128
transformer.encoder.layers.0.self_attn.value_proj.weight: 65536
transformer.encoder.layers.0.self_attn.value_proj.bias: 256
transformer.encoder.layers.0.self_attn.output_proj.weight: 65536
transformer.encoder.layers.0.self_attn.output_proj.bias: 256
transformer.encoder.layers.0.norm1.weight: 256
transformer.encoder.layers.0.norm1.bias: 256
transformer.encoder.layers.0.linear1.weight: 524288
transformer.encoder.layers.0.linear1.bias: 2048
transformer.encoder.layers.0.linear2.weight: 524288
transformer.encoder.layers.0.linear2.bias: 256
transformer.encoder.layers.0.norm2.weight: 256
transformer.encoder.layers.0.norm2.bias: 256
transformer.encoder.layers.1.pre_attention.in_proj_weight: 196608
transformer.encoder.layers.1.pre_attention.in_proj_bias: 768
transformer.encoder.layers.1.pre_attention.out_proj.weight: 65536
transformer.encoder.layers.1.pre_attention.out_proj.bias: 256
transformer.encoder.layers.1.pre_norm.weight: 256
transformer.encoder.layers.1.pre_norm.bias: 256
transformer.encoder.layers.1.self_attn.sampling_offsets.weight: 65536
transformer.encoder.layers.1.self_attn.sampling_offsets.bias: 256
transformer.encoder.layers.1.self_attn.attention_weights.weight: 32768
transformer.encoder.layers.1.self_attn.attention_weights.bias: 128
transformer.encoder.layers.1.self_attn.value_proj.weight: 65536
transformer.encoder.layers.1.self_attn.value_proj.bias: 256
transformer.encoder.layers.1.self_attn.output_proj.weight: 65536
transformer.encoder.layers.1.self_attn.output_proj.bias: 256
transformer.encoder.layers.1.norm1.weight: 256
transformer.encoder.layers.1.norm1.bias: 256
transformer.encoder.layers.1.linear1.weight: 524288
transformer.encoder.layers.1.linear1.bias: 2048
transformer.encoder.layers.1.linear2.weight: 524288
transformer.encoder.layers.1.linear2.bias: 256
transformer.encoder.layers.1.norm2.weight: 256
transformer.encoder.layers.1.norm2.bias: 256
transformer.encoder.layers.2.pre_attention.in_proj_weight: 196608
transformer.encoder.layers.2.pre_attention.in_proj_bias: 768
transformer.encoder.layers.2.pre_attention.out_proj.weight: 65536
transformer.encoder.layers.2.pre_attention.out_proj.bias: 256
transformer.encoder.layers.2.pre_norm.weight: 256
transformer.encoder.layers.2.pre_norm.bias: 256
transformer.encoder.layers.2.self_attn.sampling_offsets.weight: 65536
transformer.encoder.layers.2.self_attn.sampling_offsets.bias: 256
transformer.encoder.layers.2.self_attn.attention_weights.weight: 32768
transformer.encoder.layers.2.self_attn.attention_weights.bias: 128
transformer.encoder.layers.2.self_attn.value_proj.weight: 65536
transformer.encoder.layers.2.self_attn.value_proj.bias: 256
transformer.encoder.layers.2.self_attn.output_proj.weight: 65536
transformer.encoder.layers.2.self_attn.output_proj.bias: 256
transformer.encoder.layers.2.norm1.weight: 256
transformer.encoder.layers.2.norm1.bias: 256
transformer.encoder.layers.2.linear1.weight: 524288
transformer.encoder.layers.2.linear1.bias: 2048
transformer.encoder.layers.2.linear2.weight: 524288
transformer.encoder.layers.2.linear2.bias: 256
transformer.encoder.layers.2.norm2.weight: 256
transformer.encoder.layers.2.norm2.bias: 256
transformer.encoder.layers.3.pre_attention.in_proj_weight: 196608
transformer.encoder.layers.3.pre_attention.in_proj_bias: 768
transformer.encoder.layers.3.pre_attention.out_proj.weight: 65536
transformer.encoder.layers.3.pre_attention.out_proj.bias: 256
transformer.encoder.layers.3.pre_norm.weight: 256
transformer.encoder.layers.3.pre_norm.bias: 256
transformer.encoder.layers.3.self_attn.sampling_offsets.weight: 65536
transformer.encoder.layers.3.self_attn.sampling_offsets.bias: 256
transformer.encoder.layers.3.self_attn.attention_weights.weight: 32768
transformer.encoder.layers.3.self_attn.attention_weights.bias: 128
transformer.encoder.layers.3.self_attn.value_proj.weight: 65536
transformer.encoder.layers.3.self_attn.value_proj.bias: 256
transformer.encoder.layers.3.self_attn.output_proj.weight: 65536
transformer.encoder.layers.3.self_attn.output_proj.bias: 256
transformer.encoder.layers.3.norm1.weight: 256
transformer.encoder.layers.3.norm1.bias: 256
transformer.encoder.layers.3.linear1.weight: 524288
transformer.encoder.layers.3.linear1.bias: 2048
transformer.encoder.layers.3.linear2.weight: 524288
transformer.encoder.layers.3.linear2.bias: 256
transformer.encoder.layers.3.norm2.weight: 256
transformer.encoder.layers.3.norm2.bias: 256
transformer.encoder.layers.4.pre_attention.in_proj_weight: 196608
transformer.encoder.layers.4.pre_attention.in_proj_bias: 768
transformer.encoder.layers.4.pre_attention.out_proj.weight: 65536
transformer.encoder.layers.4.pre_attention.out_proj.bias: 256
transformer.encoder.layers.4.pre_norm.weight: 256
transformer.encoder.layers.4.pre_norm.bias: 256
transformer.encoder.layers.4.self_attn.sampling_offsets.weight: 65536
transformer.encoder.layers.4.self_attn.sampling_offsets.bias: 256
transformer.encoder.layers.4.self_attn.attention_weights.weight: 32768
transformer.encoder.layers.4.self_attn.attention_weights.bias: 128
transformer.encoder.layers.4.self_attn.value_proj.weight: 65536
transformer.encoder.layers.4.self_attn.value_proj.bias: 256
transformer.encoder.layers.4.self_attn.output_proj.weight: 65536
transformer.encoder.layers.4.self_attn.output_proj.bias: 256
transformer.encoder.layers.4.norm1.weight: 256
transformer.encoder.layers.4.norm1.bias: 256
transformer.encoder.layers.4.linear1.weight: 524288
transformer.encoder.layers.4.linear1.bias: 2048
transformer.encoder.layers.4.linear2.weight: 524288
transformer.encoder.layers.4.linear2.bias: 256
transformer.encoder.layers.4.norm2.weight: 256
transformer.encoder.layers.4.norm2.bias: 256
transformer.encoder.layers.5.pre_attention.in_proj_weight: 196608
transformer.encoder.layers.5.pre_attention.in_proj_bias: 768
transformer.encoder.layers.5.pre_attention.out_proj.weight: 65536
transformer.encoder.layers.5.pre_attention.out_proj.bias: 256
transformer.encoder.layers.5.pre_norm.weight: 256
transformer.encoder.layers.5.pre_norm.bias: 256
transformer.encoder.layers.5.self_attn.sampling_offsets.weight: 65536
transformer.encoder.layers.5.self_attn.sampling_offsets.bias: 256
transformer.encoder.layers.5.self_attn.attention_weights.weight: 32768
transformer.encoder.layers.5.self_attn.attention_weights.bias: 128
transformer.encoder.layers.5.self_attn.value_proj.weight: 65536
transformer.encoder.layers.5.self_attn.value_proj.bias: 256
transformer.encoder.layers.5.self_attn.output_proj.weight: 65536
transformer.encoder.layers.5.self_attn.output_proj.bias: 256
transformer.encoder.layers.5.norm1.weight: 256
transformer.encoder.layers.5.norm1.bias: 256
transformer.encoder.layers.5.linear1.weight: 524288
transformer.encoder.layers.5.linear1.bias: 2048
transformer.encoder.layers.5.linear2.weight: 524288
transformer.encoder.layers.5.linear2.bias: 256
transformer.encoder.layers.5.norm2.weight: 256
transformer.encoder.layers.5.norm2.bias: 256
transformer.encoder.background_embedding.row_embed.weight: 25600
transformer.encoder.background_embedding.col_embed.weight: 25600
transformer.encoder.enhance_mcsp.weight: 23296
transformer.encoder.enhance_mcsp.bias: 91
transformer.neck.lateral_convs.0.0.base_conv.weight: 256
transformer.neck.lateral_convs.0.0.base_scale.weight: 256
transformer.neck.lateral_convs.0.0.wavelet_convs.0.weight: 1024
transformer.neck.lateral_convs.0.0.wavelet_scale.0.weight: 1024
transformer.neck.lateral_convs.0.1.weight: 256
transformer.neck.lateral_convs.0.1.bias: 256
transformer.neck.lateral_convs.1.0.base_conv.weight: 256
transformer.neck.lateral_convs.1.0.base_scale.weight: 256
transformer.neck.lateral_convs.1.0.wavelet_convs.0.weight: 1024
transformer.neck.lateral_convs.1.0.wavelet_scale.0.weight: 1024
transformer.neck.lateral_convs.1.1.weight: 256
transformer.neck.lateral_convs.1.1.bias: 256
transformer.neck.lateral_convs.2.0.base_conv.weight: 256
transformer.neck.lateral_convs.2.0.base_scale.weight: 256
transformer.neck.lateral_convs.2.0.wavelet_convs.0.weight: 1024
transformer.neck.lateral_convs.2.0.wavelet_scale.0.weight: 1024
transformer.neck.lateral_convs.2.1.weight: 256
transformer.neck.lateral_convs.2.1.bias: 256
transformer.neck.layer_blocks.0.conv1.0.weight: 131072
transformer.neck.layer_blocks.0.conv1.1.weight: 256
transformer.neck.layer_blocks.0.conv1.1.bias: 256
transformer.neck.layer_blocks.0.conv2.0.weight: 131072
transformer.neck.layer_blocks.0.conv2.1.weight: 256
transformer.neck.layer_blocks.0.conv2.1.bias: 256
transformer.neck.layer_blocks.0.bottlenecks.0.conv1.0.base_conv.weight: 2304
transformer.neck.layer_blocks.0.bottlenecks.0.conv1.0.base_scale.weight: 256
transformer.neck.layer_blocks.0.bottlenecks.0.conv1.0.wavelet_convs.0.weight: 9216
transformer.neck.layer_blocks.0.bottlenecks.0.conv1.0.wavelet_scale.0.weight: 1024
transformer.neck.layer_blocks.0.bottlenecks.0.conv1.1.weight: 256
transformer.neck.layer_blocks.0.bottlenecks.0.conv1.1.bias: 256
transformer.neck.layer_blocks.0.bottlenecks.0.conv2.0.base_conv.weight: 256
transformer.neck.layer_blocks.0.bottlenecks.0.conv2.0.base_scale.weight: 256
transformer.neck.layer_blocks.0.bottlenecks.0.conv2.0.wavelet_convs.0.weight: 1024
transformer.neck.layer_blocks.0.bottlenecks.0.conv2.0.wavelet_scale.0.weight: 1024
transformer.neck.layer_blocks.0.bottlenecks.0.conv2.1.weight: 256
transformer.neck.layer_blocks.0.bottlenecks.0.conv2.1.bias: 256
transformer.neck.layer_blocks.0.bottlenecks.0.se_module.conv_mask.weight: 256
transformer.neck.layer_blocks.0.bottlenecks.0.se_module.conv_mask.bias: 1
transformer.neck.layer_blocks.0.bottlenecks.0.se_module.se_module.0.weight: 4096
transformer.neck.layer_blocks.0.bottlenecks.0.se_module.se_module.2.weight: 4096
transformer.neck.layer_blocks.0.bottlenecks.1.conv1.0.base_conv.weight: 2304
transformer.neck.layer_blocks.0.bottlenecks.1.conv1.0.base_scale.weight: 256
transformer.neck.layer_blocks.0.bottlenecks.1.conv1.0.wavelet_convs.0.weight: 9216
transformer.neck.layer_blocks.0.bottlenecks.1.conv1.0.wavelet_scale.0.weight: 1024
transformer.neck.layer_blocks.0.bottlenecks.1.conv1.1.weight: 256
transformer.neck.layer_blocks.0.bottlenecks.1.conv1.1.bias: 256
transformer.neck.layer_blocks.0.bottlenecks.1.conv2.0.base_conv.weight: 256
transformer.neck.layer_blocks.0.bottlenecks.1.conv2.0.base_scale.weight: 256
transformer.neck.layer_blocks.0.bottlenecks.1.conv2.0.wavelet_convs.0.weight: 1024
transformer.neck.layer_blocks.0.bottlenecks.1.conv2.0.wavelet_scale.0.weight: 1024
transformer.neck.layer_blocks.0.bottlenecks.1.conv2.1.weight: 256
transformer.neck.layer_blocks.0.bottlenecks.1.conv2.1.bias: 256
transformer.neck.layer_blocks.0.bottlenecks.1.se_module.conv_mask.weight: 256
transformer.neck.layer_blocks.0.bottlenecks.1.se_module.conv_mask.bias: 1
transformer.neck.layer_blocks.0.bottlenecks.1.se_module.se_module.0.weight: 4096
transformer.neck.layer_blocks.0.bottlenecks.1.se_module.se_module.2.weight: 4096
transformer.neck.layer_blocks.0.bottlenecks.2.conv1.0.base_conv.weight: 2304
transformer.neck.layer_blocks.0.bottlenecks.2.conv1.0.base_scale.weight: 256
transformer.neck.layer_blocks.0.bottlenecks.2.conv1.0.wavelet_convs.0.weight: 9216
transformer.neck.layer_blocks.0.bottlenecks.2.conv1.0.wavelet_scale.0.weight: 1024
transformer.neck.layer_blocks.0.bottlenecks.2.conv1.1.weight: 256
transformer.neck.layer_blocks.0.bottlenecks.2.conv1.1.bias: 256
transformer.neck.layer_blocks.0.bottlenecks.2.conv2.0.base_conv.weight: 256
transformer.neck.layer_blocks.0.bottlenecks.2.conv2.0.base_scale.weight: 256
transformer.neck.layer_blocks.0.bottlenecks.2.conv2.0.wavelet_convs.0.weight: 1024
transformer.neck.layer_blocks.0.bottlenecks.2.conv2.0.wavelet_scale.0.weight: 1024
transformer.neck.layer_blocks.0.bottlenecks.2.conv2.1.weight: 256
transformer.neck.layer_blocks.0.bottlenecks.2.conv2.1.bias: 256
transformer.neck.layer_blocks.0.bottlenecks.2.se_module.conv_mask.weight: 256
transformer.neck.layer_blocks.0.bottlenecks.2.se_module.conv_mask.bias: 1
transformer.neck.layer_blocks.0.bottlenecks.2.se_module.se_module.0.weight: 4096
transformer.neck.layer_blocks.0.bottlenecks.2.se_module.se_module.2.weight: 4096
transformer.neck.layer_blocks.1.conv1.0.weight: 131072
transformer.neck.layer_blocks.1.conv1.1.weight: 256
transformer.neck.layer_blocks.1.conv1.1.bias: 256
transformer.neck.layer_blocks.1.conv2.0.weight: 131072
transformer.neck.layer_blocks.1.conv2.1.weight: 256
transformer.neck.layer_blocks.1.conv2.1.bias: 256
transformer.neck.layer_blocks.1.bottlenecks.0.conv1.0.base_conv.weight: 2304
transformer.neck.layer_blocks.1.bottlenecks.0.conv1.0.base_scale.weight: 256
transformer.neck.layer_blocks.1.bottlenecks.0.conv1.0.wavelet_convs.0.weight: 9216
transformer.neck.layer_blocks.1.bottlenecks.0.conv1.0.wavelet_scale.0.weight: 1024
transformer.neck.layer_blocks.1.bottlenecks.0.conv1.1.weight: 256
transformer.neck.layer_blocks.1.bottlenecks.0.conv1.1.bias: 256
transformer.neck.layer_blocks.1.bottlenecks.0.conv2.0.base_conv.weight: 256
transformer.neck.layer_blocks.1.bottlenecks.0.conv2.0.base_scale.weight: 256
transformer.neck.layer_blocks.1.bottlenecks.0.conv2.0.wavelet_convs.0.weight: 1024
transformer.neck.layer_blocks.1.bottlenecks.0.conv2.0.wavelet_scale.0.weight: 1024
transformer.neck.layer_blocks.1.bottlenecks.0.conv2.1.weight: 256
transformer.neck.layer_blocks.1.bottlenecks.0.conv2.1.bias: 256
transformer.neck.layer_blocks.1.bottlenecks.0.se_module.conv_mask.weight: 256
transformer.neck.layer_blocks.1.bottlenecks.0.se_module.conv_mask.bias: 1
transformer.neck.layer_blocks.1.bottlenecks.0.se_module.se_module.0.weight: 4096
transformer.neck.layer_blocks.1.bottlenecks.0.se_module.se_module.2.weight: 4096
transformer.neck.layer_blocks.1.bottlenecks.1.conv1.0.base_conv.weight: 2304
transformer.neck.layer_blocks.1.bottlenecks.1.conv1.0.base_scale.weight: 256
transformer.neck.layer_blocks.1.bottlenecks.1.conv1.0.wavelet_convs.0.weight: 9216
transformer.neck.layer_blocks.1.bottlenecks.1.conv1.0.wavelet_scale.0.weight: 1024
transformer.neck.layer_blocks.1.bottlenecks.1.conv1.1.weight: 256
transformer.neck.layer_blocks.1.bottlenecks.1.conv1.1.bias: 256
transformer.neck.layer_blocks.1.bottlenecks.1.conv2.0.base_conv.weight: 256
transformer.neck.layer_blocks.1.bottlenecks.1.conv2.0.base_scale.weight: 256
transformer.neck.layer_blocks.1.bottlenecks.1.conv2.0.wavelet_convs.0.weight: 1024
transformer.neck.layer_blocks.1.bottlenecks.1.conv2.0.wavelet_scale.0.weight: 1024
transformer.neck.layer_blocks.1.bottlenecks.1.conv2.1.weight: 256
transformer.neck.layer_blocks.1.bottlenecks.1.conv2.1.bias: 256
transformer.neck.layer_blocks.1.bottlenecks.1.se_module.conv_mask.weight: 256
transformer.neck.layer_blocks.1.bottlenecks.1.se_module.conv_mask.bias: 1
transformer.neck.layer_blocks.1.bottlenecks.1.se_module.se_module.0.weight: 4096
transformer.neck.layer_blocks.1.bottlenecks.1.se_module.se_module.2.weight: 4096
transformer.neck.layer_blocks.1.bottlenecks.2.conv1.0.base_conv.weight: 2304
transformer.neck.layer_blocks.1.bottlenecks.2.conv1.0.base_scale.weight: 256
transformer.neck.layer_blocks.1.bottlenecks.2.conv1.0.wavelet_convs.0.weight: 9216
transformer.neck.layer_blocks.1.bottlenecks.2.conv1.0.wavelet_scale.0.weight: 1024
transformer.neck.layer_blocks.1.bottlenecks.2.conv1.1.weight: 256
transformer.neck.layer_blocks.1.bottlenecks.2.conv1.1.bias: 256
transformer.neck.layer_blocks.1.bottlenecks.2.conv2.0.base_conv.weight: 256
transformer.neck.layer_blocks.1.bottlenecks.2.conv2.0.base_scale.weight: 256
transformer.neck.layer_blocks.1.bottlenecks.2.conv2.0.wavelet_convs.0.weight: 1024
transformer.neck.layer_blocks.1.bottlenecks.2.conv2.0.wavelet_scale.0.weight: 1024
transformer.neck.layer_blocks.1.bottlenecks.2.conv2.1.weight: 256
transformer.neck.layer_blocks.1.bottlenecks.2.conv2.1.bias: 256
transformer.neck.layer_blocks.1.bottlenecks.2.se_module.conv_mask.weight: 256
transformer.neck.layer_blocks.1.bottlenecks.2.se_module.conv_mask.bias: 1
transformer.neck.layer_blocks.1.bottlenecks.2.se_module.se_module.0.weight: 4096
transformer.neck.layer_blocks.1.bottlenecks.2.se_module.se_module.2.weight: 4096
transformer.neck.layer_blocks.2.conv1.0.weight: 131072
transformer.neck.layer_blocks.2.conv1.1.weight: 256
transformer.neck.layer_blocks.2.conv1.1.bias: 256
transformer.neck.layer_blocks.2.conv2.0.weight: 131072
transformer.neck.layer_blocks.2.conv2.1.weight: 256
transformer.neck.layer_blocks.2.conv2.1.bias: 256
transformer.neck.layer_blocks.2.bottlenecks.0.conv1.0.base_conv.weight: 2304
transformer.neck.layer_blocks.2.bottlenecks.0.conv1.0.base_scale.weight: 256
transformer.neck.layer_blocks.2.bottlenecks.0.conv1.0.wavelet_convs.0.weight: 9216
transformer.neck.layer_blocks.2.bottlenecks.0.conv1.0.wavelet_scale.0.weight: 1024
transformer.neck.layer_blocks.2.bottlenecks.0.conv1.1.weight: 256
transformer.neck.layer_blocks.2.bottlenecks.0.conv1.1.bias: 256
transformer.neck.layer_blocks.2.bottlenecks.0.conv2.0.base_conv.weight: 256
transformer.neck.layer_blocks.2.bottlenecks.0.conv2.0.base_scale.weight: 256
transformer.neck.layer_blocks.2.bottlenecks.0.conv2.0.wavelet_convs.0.weight: 1024
transformer.neck.layer_blocks.2.bottlenecks.0.conv2.0.wavelet_scale.0.weight: 1024
transformer.neck.layer_blocks.2.bottlenecks.0.conv2.1.weight: 256
transformer.neck.layer_blocks.2.bottlenecks.0.conv2.1.bias: 256
transformer.neck.layer_blocks.2.bottlenecks.0.se_module.conv_mask.weight: 256
transformer.neck.layer_blocks.2.bottlenecks.0.se_module.conv_mask.bias: 1
transformer.neck.layer_blocks.2.bottlenecks.0.se_module.se_module.0.weight: 4096
transformer.neck.layer_blocks.2.bottlenecks.0.se_module.se_module.2.weight: 4096
transformer.neck.layer_blocks.2.bottlenecks.1.conv1.0.base_conv.weight: 2304
transformer.neck.layer_blocks.2.bottlenecks.1.conv1.0.base_scale.weight: 256
transformer.neck.layer_blocks.2.bottlenecks.1.conv1.0.wavelet_convs.0.weight: 9216
transformer.neck.layer_blocks.2.bottlenecks.1.conv1.0.wavelet_scale.0.weight: 1024
transformer.neck.layer_blocks.2.bottlenecks.1.conv1.1.weight: 256
transformer.neck.layer_blocks.2.bottlenecks.1.conv1.1.bias: 256
transformer.neck.layer_blocks.2.bottlenecks.1.conv2.0.base_conv.weight: 256
transformer.neck.layer_blocks.2.bottlenecks.1.conv2.0.base_scale.weight: 256
transformer.neck.layer_blocks.2.bottlenecks.1.conv2.0.wavelet_convs.0.weight: 1024
transformer.neck.layer_blocks.2.bottlenecks.1.conv2.0.wavelet_scale.0.weight: 1024
transformer.neck.layer_blocks.2.bottlenecks.1.conv2.1.weight: 256
transformer.neck.layer_blocks.2.bottlenecks.1.conv2.1.bias: 256
transformer.neck.layer_blocks.2.bottlenecks.1.se_module.conv_mask.weight: 256
transformer.neck.layer_blocks.2.bottlenecks.1.se_module.conv_mask.bias: 1
transformer.neck.layer_blocks.2.bottlenecks.1.se_module.se_module.0.weight: 4096
transformer.neck.layer_blocks.2.bottlenecks.1.se_module.se_module.2.weight: 4096
transformer.neck.layer_blocks.2.bottlenecks.2.conv1.0.base_conv.weight: 2304
transformer.neck.layer_blocks.2.bottlenecks.2.conv1.0.base_scale.weight: 256
transformer.neck.layer_blocks.2.bottlenecks.2.conv1.0.wavelet_convs.0.weight: 9216
transformer.neck.layer_blocks.2.bottlenecks.2.conv1.0.wavelet_scale.0.weight: 1024
transformer.neck.layer_blocks.2.bottlenecks.2.conv1.1.weight: 256
transformer.neck.layer_blocks.2.bottlenecks.2.conv1.1.bias: 256
transformer.neck.layer_blocks.2.bottlenecks.2.conv2.0.base_conv.weight: 256
transformer.neck.layer_blocks.2.bottlenecks.2.conv2.0.base_scale.weight: 256
transformer.neck.layer_blocks.2.bottlenecks.2.conv2.0.wavelet_convs.0.weight: 1024
transformer.neck.layer_blocks.2.bottlenecks.2.conv2.0.wavelet_scale.0.weight: 1024
transformer.neck.layer_blocks.2.bottlenecks.2.conv2.1.weight: 256
transformer.neck.layer_blocks.2.bottlenecks.2.conv2.1.bias: 256
transformer.neck.layer_blocks.2.bottlenecks.2.se_module.conv_mask.weight: 256
transformer.neck.layer_blocks.2.bottlenecks.2.se_module.conv_mask.bias: 1
transformer.neck.layer_blocks.2.bottlenecks.2.se_module.se_module.0.weight: 4096
transformer.neck.layer_blocks.2.bottlenecks.2.se_module.se_module.2.weight: 4096
transformer.neck.downsample_blocks.0.0.base_conv.weight: 2304
transformer.neck.downsample_blocks.0.0.base_scale.weight: 256
transformer.neck.downsample_blocks.0.0.wavelet_convs.0.weight: 9216
transformer.neck.downsample_blocks.0.0.wavelet_scale.0.weight: 1024
transformer.neck.downsample_blocks.0.1.weight: 256
transformer.neck.downsample_blocks.0.1.bias: 256
transformer.neck.downsample_blocks.1.0.base_conv.weight: 2304
transformer.neck.downsample_blocks.1.0.base_scale.weight: 256
transformer.neck.downsample_blocks.1.0.wavelet_convs.0.weight: 9216
transformer.neck.downsample_blocks.1.0.wavelet_scale.0.weight: 1024
transformer.neck.downsample_blocks.1.1.weight: 256
transformer.neck.downsample_blocks.1.1.bias: 256
transformer.neck.downsample_blocks.2.0.base_conv.weight: 2304
transformer.neck.downsample_blocks.2.0.base_scale.weight: 256
transformer.neck.downsample_blocks.2.0.wavelet_convs.0.weight: 9216
transformer.neck.downsample_blocks.2.0.wavelet_scale.0.weight: 1024
transformer.neck.downsample_blocks.2.1.weight: 256
transformer.neck.downsample_blocks.2.1.bias: 256
transformer.neck.pan_blocks.0.conv1.0.weight: 131072
transformer.neck.pan_blocks.0.conv1.1.weight: 256
transformer.neck.pan_blocks.0.conv1.1.bias: 256
transformer.neck.pan_blocks.0.conv2.0.weight: 131072
transformer.neck.pan_blocks.0.conv2.1.weight: 256
transformer.neck.pan_blocks.0.conv2.1.bias: 256
transformer.neck.pan_blocks.0.bottlenecks.0.conv1.0.base_conv.weight: 2304
transformer.neck.pan_blocks.0.bottlenecks.0.conv1.0.base_scale.weight: 256
transformer.neck.pan_blocks.0.bottlenecks.0.conv1.0.wavelet_convs.0.weight: 9216
transformer.neck.pan_blocks.0.bottlenecks.0.conv1.0.wavelet_scale.0.weight: 1024
transformer.neck.pan_blocks.0.bottlenecks.0.conv1.1.weight: 256
transformer.neck.pan_blocks.0.bottlenecks.0.conv1.1.bias: 256
transformer.neck.pan_blocks.0.bottlenecks.0.conv2.0.base_conv.weight: 256
transformer.neck.pan_blocks.0.bottlenecks.0.conv2.0.base_scale.weight: 256
transformer.neck.pan_blocks.0.bottlenecks.0.conv2.0.wavelet_convs.0.weight: 1024
transformer.neck.pan_blocks.0.bottlenecks.0.conv2.0.wavelet_scale.0.weight: 1024
transformer.neck.pan_blocks.0.bottlenecks.0.conv2.1.weight: 256
transformer.neck.pan_blocks.0.bottlenecks.0.conv2.1.bias: 256
transformer.neck.pan_blocks.0.bottlenecks.0.se_module.conv_mask.weight: 256
transformer.neck.pan_blocks.0.bottlenecks.0.se_module.conv_mask.bias: 1
transformer.neck.pan_blocks.0.bottlenecks.0.se_module.se_module.0.weight: 4096
transformer.neck.pan_blocks.0.bottlenecks.0.se_module.se_module.2.weight: 4096
transformer.neck.pan_blocks.0.bottlenecks.1.conv1.0.base_conv.weight: 2304
transformer.neck.pan_blocks.0.bottlenecks.1.conv1.0.base_scale.weight: 256
transformer.neck.pan_blocks.0.bottlenecks.1.conv1.0.wavelet_convs.0.weight: 9216
transformer.neck.pan_blocks.0.bottlenecks.1.conv1.0.wavelet_scale.0.weight: 1024
transformer.neck.pan_blocks.0.bottlenecks.1.conv1.1.weight: 256
transformer.neck.pan_blocks.0.bottlenecks.1.conv1.1.bias: 256
transformer.neck.pan_blocks.0.bottlenecks.1.conv2.0.base_conv.weight: 256
transformer.neck.pan_blocks.0.bottlenecks.1.conv2.0.base_scale.weight: 256
transformer.neck.pan_blocks.0.bottlenecks.1.conv2.0.wavelet_convs.0.weight: 1024
transformer.neck.pan_blocks.0.bottlenecks.1.conv2.0.wavelet_scale.0.weight: 1024
transformer.neck.pan_blocks.0.bottlenecks.1.conv2.1.weight: 256
transformer.neck.pan_blocks.0.bottlenecks.1.conv2.1.bias: 256
transformer.neck.pan_blocks.0.bottlenecks.1.se_module.conv_mask.weight: 256
transformer.neck.pan_blocks.0.bottlenecks.1.se_module.conv_mask.bias: 1
transformer.neck.pan_blocks.0.bottlenecks.1.se_module.se_module.0.weight: 4096
transformer.neck.pan_blocks.0.bottlenecks.1.se_module.se_module.2.weight: 4096
transformer.neck.pan_blocks.0.bottlenecks.2.conv1.0.base_conv.weight: 2304
transformer.neck.pan_blocks.0.bottlenecks.2.conv1.0.base_scale.weight: 256
transformer.neck.pan_blocks.0.bottlenecks.2.conv1.0.wavelet_convs.0.weight: 9216
transformer.neck.pan_blocks.0.bottlenecks.2.conv1.0.wavelet_scale.0.weight: 1024
transformer.neck.pan_blocks.0.bottlenecks.2.conv1.1.weight: 256
transformer.neck.pan_blocks.0.bottlenecks.2.conv1.1.bias: 256
transformer.neck.pan_blocks.0.bottlenecks.2.conv2.0.base_conv.weight: 256
transformer.neck.pan_blocks.0.bottlenecks.2.conv2.0.base_scale.weight: 256
transformer.neck.pan_blocks.0.bottlenecks.2.conv2.0.wavelet_convs.0.weight: 1024
transformer.neck.pan_blocks.0.bottlenecks.2.conv2.0.wavelet_scale.0.weight: 1024
transformer.neck.pan_blocks.0.bottlenecks.2.conv2.1.weight: 256
transformer.neck.pan_blocks.0.bottlenecks.2.conv2.1.bias: 256
transformer.neck.pan_blocks.0.bottlenecks.2.se_module.conv_mask.weight: 256
transformer.neck.pan_blocks.0.bottlenecks.2.se_module.conv_mask.bias: 1
transformer.neck.pan_blocks.0.bottlenecks.2.se_module.se_module.0.weight: 4096
transformer.neck.pan_blocks.0.bottlenecks.2.se_module.se_module.2.weight: 4096
transformer.neck.pan_blocks.1.conv1.0.weight: 131072
transformer.neck.pan_blocks.1.conv1.1.weight: 256
transformer.neck.pan_blocks.1.conv1.1.bias: 256
transformer.neck.pan_blocks.1.conv2.0.weight: 131072
transformer.neck.pan_blocks.1.conv2.1.weight: 256
transformer.neck.pan_blocks.1.conv2.1.bias: 256
transformer.neck.pan_blocks.1.bottlenecks.0.conv1.0.base_conv.weight: 2304
transformer.neck.pan_blocks.1.bottlenecks.0.conv1.0.base_scale.weight: 256
transformer.neck.pan_blocks.1.bottlenecks.0.conv1.0.wavelet_convs.0.weight: 9216
transformer.neck.pan_blocks.1.bottlenecks.0.conv1.0.wavelet_scale.0.weight: 1024
transformer.neck.pan_blocks.1.bottlenecks.0.conv1.1.weight: 256
transformer.neck.pan_blocks.1.bottlenecks.0.conv1.1.bias: 256
transformer.neck.pan_blocks.1.bottlenecks.0.conv2.0.base_conv.weight: 256
transformer.neck.pan_blocks.1.bottlenecks.0.conv2.0.base_scale.weight: 256
transformer.neck.pan_blocks.1.bottlenecks.0.conv2.0.wavelet_convs.0.weight: 1024
transformer.neck.pan_blocks.1.bottlenecks.0.conv2.0.wavelet_scale.0.weight: 1024
transformer.neck.pan_blocks.1.bottlenecks.0.conv2.1.weight: 256
transformer.neck.pan_blocks.1.bottlenecks.0.conv2.1.bias: 256
transformer.neck.pan_blocks.1.bottlenecks.0.se_module.conv_mask.weight: 256
transformer.neck.pan_blocks.1.bottlenecks.0.se_module.conv_mask.bias: 1
transformer.neck.pan_blocks.1.bottlenecks.0.se_module.se_module.0.weight: 4096
transformer.neck.pan_blocks.1.bottlenecks.0.se_module.se_module.2.weight: 4096
transformer.neck.pan_blocks.1.bottlenecks.1.conv1.0.base_conv.weight: 2304
transformer.neck.pan_blocks.1.bottlenecks.1.conv1.0.base_scale.weight: 256
transformer.neck.pan_blocks.1.bottlenecks.1.conv1.0.wavelet_convs.0.weight: 9216
transformer.neck.pan_blocks.1.bottlenecks.1.conv1.0.wavelet_scale.0.weight: 1024
transformer.neck.pan_blocks.1.bottlenecks.1.conv1.1.weight: 256
transformer.neck.pan_blocks.1.bottlenecks.1.conv1.1.bias: 256
transformer.neck.pan_blocks.1.bottlenecks.1.conv2.0.base_conv.weight: 256
transformer.neck.pan_blocks.1.bottlenecks.1.conv2.0.base_scale.weight: 256
transformer.neck.pan_blocks.1.bottlenecks.1.conv2.0.wavelet_convs.0.weight: 1024
transformer.neck.pan_blocks.1.bottlenecks.1.conv2.0.wavelet_scale.0.weight: 1024
transformer.neck.pan_blocks.1.bottlenecks.1.conv2.1.weight: 256
transformer.neck.pan_blocks.1.bottlenecks.1.conv2.1.bias: 256
transformer.neck.pan_blocks.1.bottlenecks.1.se_module.conv_mask.weight: 256
transformer.neck.pan_blocks.1.bottlenecks.1.se_module.conv_mask.bias: 1
transformer.neck.pan_blocks.1.bottlenecks.1.se_module.se_module.0.weight: 4096
transformer.neck.pan_blocks.1.bottlenecks.1.se_module.se_module.2.weight: 4096
transformer.neck.pan_blocks.1.bottlenecks.2.conv1.0.base_conv.weight: 2304
transformer.neck.pan_blocks.1.bottlenecks.2.conv1.0.base_scale.weight: 256
transformer.neck.pan_blocks.1.bottlenecks.2.conv1.0.wavelet_convs.0.weight: 9216
transformer.neck.pan_blocks.1.bottlenecks.2.conv1.0.wavelet_scale.0.weight: 1024
transformer.neck.pan_blocks.1.bottlenecks.2.conv1.1.weight: 256
transformer.neck.pan_blocks.1.bottlenecks.2.conv1.1.bias: 256
transformer.neck.pan_blocks.1.bottlenecks.2.conv2.0.base_conv.weight: 256
transformer.neck.pan_blocks.1.bottlenecks.2.conv2.0.base_scale.weight: 256
transformer.neck.pan_blocks.1.bottlenecks.2.conv2.0.wavelet_convs.0.weight: 1024
transformer.neck.pan_blocks.1.bottlenecks.2.conv2.0.wavelet_scale.0.weight: 1024
transformer.neck.pan_blocks.1.bottlenecks.2.conv2.1.weight: 256
transformer.neck.pan_blocks.1.bottlenecks.2.conv2.1.bias: 256
transformer.neck.pan_blocks.1.bottlenecks.2.se_module.conv_mask.weight: 256
transformer.neck.pan_blocks.1.bottlenecks.2.se_module.conv_mask.bias: 1
transformer.neck.pan_blocks.1.bottlenecks.2.se_module.se_module.0.weight: 4096
transformer.neck.pan_blocks.1.bottlenecks.2.se_module.se_module.2.weight: 4096
transformer.neck.pan_blocks.2.conv1.0.weight: 131072
transformer.neck.pan_blocks.2.conv1.1.weight: 256
transformer.neck.pan_blocks.2.conv1.1.bias: 256
transformer.neck.pan_blocks.2.conv2.0.weight: 131072
transformer.neck.pan_blocks.2.conv2.1.weight: 256
transformer.neck.pan_blocks.2.conv2.1.bias: 256
transformer.neck.pan_blocks.2.bottlenecks.0.conv1.0.base_conv.weight: 2304
transformer.neck.pan_blocks.2.bottlenecks.0.conv1.0.base_scale.weight: 256
transformer.neck.pan_blocks.2.bottlenecks.0.conv1.0.wavelet_convs.0.weight: 9216
transformer.neck.pan_blocks.2.bottlenecks.0.conv1.0.wavelet_scale.0.weight: 1024
transformer.neck.pan_blocks.2.bottlenecks.0.conv1.1.weight: 256
transformer.neck.pan_blocks.2.bottlenecks.0.conv1.1.bias: 256
transformer.neck.pan_blocks.2.bottlenecks.0.conv2.0.base_conv.weight: 256
transformer.neck.pan_blocks.2.bottlenecks.0.conv2.0.base_scale.weight: 256
transformer.neck.pan_blocks.2.bottlenecks.0.conv2.0.wavelet_convs.0.weight: 1024
transformer.neck.pan_blocks.2.bottlenecks.0.conv2.0.wavelet_scale.0.weight: 1024
transformer.neck.pan_blocks.2.bottlenecks.0.conv2.1.weight: 256
transformer.neck.pan_blocks.2.bottlenecks.0.conv2.1.bias: 256
transformer.neck.pan_blocks.2.bottlenecks.0.se_module.conv_mask.weight: 256
transformer.neck.pan_blocks.2.bottlenecks.0.se_module.conv_mask.bias: 1
transformer.neck.pan_blocks.2.bottlenecks.0.se_module.se_module.0.weight: 4096
transformer.neck.pan_blocks.2.bottlenecks.0.se_module.se_module.2.weight: 4096
transformer.neck.pan_blocks.2.bottlenecks.1.conv1.0.base_conv.weight: 2304
transformer.neck.pan_blocks.2.bottlenecks.1.conv1.0.base_scale.weight: 256
transformer.neck.pan_blocks.2.bottlenecks.1.conv1.0.wavelet_convs.0.weight: 9216
transformer.neck.pan_blocks.2.bottlenecks.1.conv1.0.wavelet_scale.0.weight: 1024
transformer.neck.pan_blocks.2.bottlenecks.1.conv1.1.weight: 256
transformer.neck.pan_blocks.2.bottlenecks.1.conv1.1.bias: 256
transformer.neck.pan_blocks.2.bottlenecks.1.conv2.0.base_conv.weight: 256
transformer.neck.pan_blocks.2.bottlenecks.1.conv2.0.base_scale.weight: 256
transformer.neck.pan_blocks.2.bottlenecks.1.conv2.0.wavelet_convs.0.weight: 1024
transformer.neck.pan_blocks.2.bottlenecks.1.conv2.0.wavelet_scale.0.weight: 1024
transformer.neck.pan_blocks.2.bottlenecks.1.conv2.1.weight: 256
transformer.neck.pan_blocks.2.bottlenecks.1.conv2.1.bias: 256
transformer.neck.pan_blocks.2.bottlenecks.1.se_module.conv_mask.weight: 256
transformer.neck.pan_blocks.2.bottlenecks.1.se_module.conv_mask.bias: 1
transformer.neck.pan_blocks.2.bottlenecks.1.se_module.se_module.0.weight: 4096
transformer.neck.pan_blocks.2.bottlenecks.1.se_module.se_module.2.weight: 4096
transformer.neck.pan_blocks.2.bottlenecks.2.conv1.0.base_conv.weight: 2304
transformer.neck.pan_blocks.2.bottlenecks.2.conv1.0.base_scale.weight: 256
transformer.neck.pan_blocks.2.bottlenecks.2.conv1.0.wavelet_convs.0.weight: 9216
transformer.neck.pan_blocks.2.bottlenecks.2.conv1.0.wavelet_scale.0.weight: 1024
transformer.neck.pan_blocks.2.bottlenecks.2.conv1.1.weight: 256
transformer.neck.pan_blocks.2.bottlenecks.2.conv1.1.bias: 256
transformer.neck.pan_blocks.2.bottlenecks.2.conv2.0.base_conv.weight: 256
transformer.neck.pan_blocks.2.bottlenecks.2.conv2.0.base_scale.weight: 256
transformer.neck.pan_blocks.2.bottlenecks.2.conv2.0.wavelet_convs.0.weight: 1024
transformer.neck.pan_blocks.2.bottlenecks.2.conv2.0.wavelet_scale.0.weight: 1024
transformer.neck.pan_blocks.2.bottlenecks.2.conv2.1.weight: 256
transformer.neck.pan_blocks.2.bottlenecks.2.conv2.1.bias: 256
transformer.neck.pan_blocks.2.bottlenecks.2.se_module.conv_mask.weight: 256
transformer.neck.pan_blocks.2.bottlenecks.2.se_module.conv_mask.bias: 1
transformer.neck.pan_blocks.2.bottlenecks.2.se_module.se_module.0.weight: 4096
transformer.neck.pan_blocks.2.bottlenecks.2.se_module.se_module.2.weight: 4096
transformer.decoder.layers.0.cross_attn.sampling_offsets.weight: 65536
transformer.decoder.layers.0.cross_attn.sampling_offsets.bias: 256
transformer.decoder.layers.0.cross_attn.attention_weights.weight: 32768
transformer.decoder.layers.0.cross_attn.attention_weights.bias: 128
transformer.decoder.layers.0.cross_attn.value_proj.weight: 65536
transformer.decoder.layers.0.cross_attn.value_proj.bias: 256
transformer.decoder.layers.0.cross_attn.output_proj.weight: 65536
transformer.decoder.layers.0.cross_attn.output_proj.bias: 256
transformer.decoder.layers.0.norm1.weight: 256
transformer.decoder.layers.0.norm1.bias: 256
transformer.decoder.layers.0.self_attn.in_proj_weight: 196608
transformer.decoder.layers.0.self_attn.in_proj_bias: 768
transformer.decoder.layers.0.self_attn.out_proj.weight: 65536
transformer.decoder.layers.0.self_attn.out_proj.bias: 256
transformer.decoder.layers.0.norm2.weight: 256
transformer.decoder.layers.0.norm2.bias: 256
transformer.decoder.layers.0.linear1.weight: 524288
transformer.decoder.layers.0.linear1.bias: 2048
transformer.decoder.layers.0.linear2.weight: 524288
transformer.decoder.layers.0.linear2.bias: 256
transformer.decoder.layers.0.norm3.weight: 256
transformer.decoder.layers.0.norm3.bias: 256
transformer.decoder.layers.1.cross_attn.sampling_offsets.weight: 65536
transformer.decoder.layers.1.cross_attn.sampling_offsets.bias: 256
transformer.decoder.layers.1.cross_attn.attention_weights.weight: 32768
transformer.decoder.layers.1.cross_attn.attention_weights.bias: 128
transformer.decoder.layers.1.cross_attn.value_proj.weight: 65536
transformer.decoder.layers.1.cross_attn.value_proj.bias: 256
transformer.decoder.layers.1.cross_attn.output_proj.weight: 65536
transformer.decoder.layers.1.cross_attn.output_proj.bias: 256
transformer.decoder.layers.1.norm1.weight: 256
transformer.decoder.layers.1.norm1.bias: 256
transformer.decoder.layers.1.self_attn.in_proj_weight: 196608
transformer.decoder.layers.1.self_attn.in_proj_bias: 768
transformer.decoder.layers.1.self_attn.out_proj.weight: 65536
transformer.decoder.layers.1.self_attn.out_proj.bias: 256
transformer.decoder.layers.1.norm2.weight: 256
transformer.decoder.layers.1.norm2.bias: 256
transformer.decoder.layers.1.linear1.weight: 524288
transformer.decoder.layers.1.linear1.bias: 2048
transformer.decoder.layers.1.linear2.weight: 524288
transformer.decoder.layers.1.linear2.bias: 256
transformer.decoder.layers.1.norm3.weight: 256
transformer.decoder.layers.1.norm3.bias: 256
transformer.decoder.layers.2.cross_attn.sampling_offsets.weight: 65536
transformer.decoder.layers.2.cross_attn.sampling_offsets.bias: 256
transformer.decoder.layers.2.cross_attn.attention_weights.weight: 32768
transformer.decoder.layers.2.cross_attn.attention_weights.bias: 128
transformer.decoder.layers.2.cross_attn.value_proj.weight: 65536
transformer.decoder.layers.2.cross_attn.value_proj.bias: 256
transformer.decoder.layers.2.cross_attn.output_proj.weight: 65536
transformer.decoder.layers.2.cross_attn.output_proj.bias: 256
transformer.decoder.layers.2.norm1.weight: 256
transformer.decoder.layers.2.norm1.bias: 256
transformer.decoder.layers.2.self_attn.in_proj_weight: 196608
transformer.decoder.layers.2.self_attn.in_proj_bias: 768
transformer.decoder.layers.2.self_attn.out_proj.weight: 65536
transformer.decoder.layers.2.self_attn.out_proj.bias: 256
transformer.decoder.layers.2.norm2.weight: 256
transformer.decoder.layers.2.norm2.bias: 256
transformer.decoder.layers.2.linear1.weight: 524288
transformer.decoder.layers.2.linear1.bias: 2048
transformer.decoder.layers.2.linear2.weight: 524288
transformer.decoder.layers.2.linear2.bias: 256
transformer.decoder.layers.2.norm3.weight: 256
transformer.decoder.layers.2.norm3.bias: 256
transformer.decoder.layers.3.cross_attn.sampling_offsets.weight: 65536
transformer.decoder.layers.3.cross_attn.sampling_offsets.bias: 256
transformer.decoder.layers.3.cross_attn.attention_weights.weight: 32768
transformer.decoder.layers.3.cross_attn.attention_weights.bias: 128
transformer.decoder.layers.3.cross_attn.value_proj.weight: 65536
transformer.decoder.layers.3.cross_attn.value_proj.bias: 256
transformer.decoder.layers.3.cross_attn.output_proj.weight: 65536
transformer.decoder.layers.3.cross_attn.output_proj.bias: 256
transformer.decoder.layers.3.norm1.weight: 256
transformer.decoder.layers.3.norm1.bias: 256
transformer.decoder.layers.3.self_attn.in_proj_weight: 196608
transformer.decoder.layers.3.self_attn.in_proj_bias: 768
transformer.decoder.layers.3.self_attn.out_proj.weight: 65536
transformer.decoder.layers.3.self_attn.out_proj.bias: 256
transformer.decoder.layers.3.norm2.weight: 256
transformer.decoder.layers.3.norm2.bias: 256
transformer.decoder.layers.3.linear1.weight: 524288
transformer.decoder.layers.3.linear1.bias: 2048
transformer.decoder.layers.3.linear2.weight: 524288
transformer.decoder.layers.3.linear2.bias: 256
transformer.decoder.layers.3.norm3.weight: 256
transformer.decoder.layers.3.norm3.bias: 256
transformer.decoder.layers.4.cross_attn.sampling_offsets.weight: 65536
transformer.decoder.layers.4.cross_attn.sampling_offsets.bias: 256
transformer.decoder.layers.4.cross_attn.attention_weights.weight: 32768
transformer.decoder.layers.4.cross_attn.attention_weights.bias: 128
transformer.decoder.layers.4.cross_attn.value_proj.weight: 65536
transformer.decoder.layers.4.cross_attn.value_proj.bias: 256
transformer.decoder.layers.4.cross_attn.output_proj.weight: 65536
transformer.decoder.layers.4.cross_attn.output_proj.bias: 256
transformer.decoder.layers.4.norm1.weight: 256
transformer.decoder.layers.4.norm1.bias: 256
transformer.decoder.layers.4.self_attn.in_proj_weight: 196608
transformer.decoder.layers.4.self_attn.in_proj_bias: 768
transformer.decoder.layers.4.self_attn.out_proj.weight: 65536
transformer.decoder.layers.4.self_attn.out_proj.bias: 256
transformer.decoder.layers.4.norm2.weight: 256
transformer.decoder.layers.4.norm2.bias: 256
transformer.decoder.layers.4.linear1.weight: 524288
transformer.decoder.layers.4.linear1.bias: 2048
transformer.decoder.layers.4.linear2.weight: 524288
transformer.decoder.layers.4.linear2.bias: 256
transformer.decoder.layers.4.norm3.weight: 256
transformer.decoder.layers.4.norm3.bias: 256
transformer.decoder.layers.5.cross_attn.sampling_offsets.weight: 65536
transformer.decoder.layers.5.cross_attn.sampling_offsets.bias: 256
transformer.decoder.layers.5.cross_attn.attention_weights.weight: 32768
transformer.decoder.layers.5.cross_attn.attention_weights.bias: 128
transformer.decoder.layers.5.cross_attn.value_proj.weight: 65536
transformer.decoder.layers.5.cross_attn.value_proj.bias: 256
transformer.decoder.layers.5.cross_attn.output_proj.weight: 65536
transformer.decoder.layers.5.cross_attn.output_proj.bias: 256
transformer.decoder.layers.5.norm1.weight: 256
transformer.decoder.layers.5.norm1.bias: 256
transformer.decoder.layers.5.self_attn.in_proj_weight: 196608
transformer.decoder.layers.5.self_attn.in_proj_bias: 768
transformer.decoder.layers.5.self_attn.out_proj.weight: 65536
transformer.decoder.layers.5.self_attn.out_proj.bias: 256
transformer.decoder.layers.5.norm2.weight: 256
transformer.decoder.layers.5.norm2.bias: 256
transformer.decoder.layers.5.linear1.weight: 524288
transformer.decoder.layers.5.linear1.bias: 2048
transformer.decoder.layers.5.linear2.weight: 524288
transformer.decoder.layers.5.linear2.bias: 256
transformer.decoder.layers.5.norm3.weight: 256
transformer.decoder.layers.5.norm3.bias: 256
transformer.decoder.ref_point_head.layers.0.weight: 131072
transformer.decoder.ref_point_head.layers.0.bias: 256
transformer.decoder.ref_point_head.layers.1.weight: 65536
transformer.decoder.ref_point_head.layers.1.bias: 256
transformer.decoder.class_head.0.weight: 23296
transformer.decoder.class_head.0.bias: 91
transformer.decoder.class_head.1.weight: 23296
transformer.decoder.class_head.1.bias: 91
transformer.decoder.class_head.2.weight: 23296
transformer.decoder.class_head.2.bias: 91
transformer.decoder.class_head.3.weight: 23296
transformer.decoder.class_head.3.bias: 91
transformer.decoder.class_head.4.weight: 23296
transformer.decoder.class_head.4.bias: 91
transformer.decoder.class_head.5.weight: 23296
transformer.decoder.class_head.5.bias: 91
transformer.decoder.bbox_head.0.layers.0.weight: 65536
transformer.decoder.bbox_head.0.layers.0.bias: 256
transformer.decoder.bbox_head.0.layers.1.weight: 65536
transformer.decoder.bbox_head.0.layers.1.bias: 256
transformer.decoder.bbox_head.0.layers.2.weight: 1024
transformer.decoder.bbox_head.0.layers.2.bias: 4
transformer.decoder.bbox_head.1.layers.0.weight: 65536
transformer.decoder.bbox_head.1.layers.0.bias: 256
transformer.decoder.bbox_head.1.layers.1.weight: 65536
transformer.decoder.bbox_head.1.layers.1.bias: 256
transformer.decoder.bbox_head.1.layers.2.weight: 1024
transformer.decoder.bbox_head.1.layers.2.bias: 4
transformer.decoder.bbox_head.2.layers.0.weight: 65536
transformer.decoder.bbox_head.2.layers.0.bias: 256
transformer.decoder.bbox_head.2.layers.1.weight: 65536
transformer.decoder.bbox_head.2.layers.1.bias: 256
transformer.decoder.bbox_head.2.layers.2.weight: 1024
transformer.decoder.bbox_head.2.layers.2.bias: 4
transformer.decoder.bbox_head.3.layers.0.weight: 65536
transformer.decoder.bbox_head.3.layers.0.bias: 256
transformer.decoder.bbox_head.3.layers.1.weight: 65536
transformer.decoder.bbox_head.3.layers.1.bias: 256
transformer.decoder.bbox_head.3.layers.2.weight: 1024
transformer.decoder.bbox_head.3.layers.2.bias: 4
transformer.decoder.bbox_head.4.layers.0.weight: 65536
transformer.decoder.bbox_head.4.layers.0.bias: 256
transformer.decoder.bbox_head.4.layers.1.weight: 65536
transformer.decoder.bbox_head.4.layers.1.bias: 256
transformer.decoder.bbox_head.4.layers.2.weight: 1024
transformer.decoder.bbox_head.4.layers.2.bias: 4
transformer.decoder.bbox_head.5.layers.0.weight: 65536
transformer.decoder.bbox_head.5.layers.0.bias: 256
transformer.decoder.bbox_head.5.layers.1.weight: 65536
transformer.decoder.bbox_head.5.layers.1.bias: 256
transformer.decoder.bbox_head.5.layers.2.weight: 1024
transformer.decoder.bbox_head.5.layers.2.bias: 4
transformer.decoder.norm.weight: 256
transformer.decoder.norm.bias: 256
transformer.tgt_embed.weight: 230400
transformer.encoder_bbox_head.layers.0.weight: 65536
transformer.encoder_bbox_head.layers.0.bias: 256
transformer.encoder_bbox_head.layers.1.weight: 65536
transformer.encoder_bbox_head.layers.1.bias: 256
transformer.encoder_bbox_head.layers.2.weight: 1024
transformer.encoder_bbox_head.layers.2.bias: 4
transformer.enc_mask_predictor.layer1.0.weight: 256
transformer.enc_mask_predictor.layer1.0.bias: 256
transformer.enc_mask_predictor.layer1.1.weight: 65536
transformer.enc_mask_predictor.layer1.1.bias: 256
transformer.enc_mask_predictor.layer2.0.weight: 32768
transformer.enc_mask_predictor.layer2.0.bias: 128
transformer.enc_mask_predictor.layer2.2.weight: 8192
transformer.enc_mask_predictor.layer2.2.bias: 64
transformer.enc_mask_predictor.layer2.4.weight: 64
transformer.enc_mask_predictor.layer2.4.bias: 1
denoising_generator.label_encoder.weight: 23296
Parameters required_grad count: 51265455
