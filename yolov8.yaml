# Ultralytics YOLO 🚀, AGPL-3.0 license
# YOLOv8 object detection model with P3-P5 outputs. For Usage examples see https://docs.ultralytics.com/tasks/detect

# Parameters
nc: 3  # number of classes#标签数
scales: # model compound scaling constants, i.e. 'model=yolov8n.yaml' will call yolov8.yaml with scale 'n'
  # [depth, width, max_channels]
  s: [0.33, 0.50, 1024]  # YOLOv8s summary: 225 layers, 11166560 parameters, 11166544 gradients,  28.8 GFLOPs


# YOLOv8.0n backbone
backbone:
  # [from, repeats, module, args]
  - [-1, 1, Conv, [64, 3, 2]]  # 0-P1/2
  - [-1, 1, Conv, [128, 3, 2]]  # 1-P2/4
  - [-1, 3, C2f, [128, True]]
  - [-1, 1, Conv, [256, 3, 2]]  # 3-P3/8
  - [-1, 6, C2f, [256, True]]
  - [-1, 1, Conv, [512, 3, 2]]  # 5-P4/16
  - [-1, 6, C2f, [512, True]]
  - [-1, 1, Conv, [1024, 3, 2]]  # 7-P5/32
  - [-1, 3, C2f, [1024, True]]
  - [-1, 1, SPPF, [1024, 5]]  # 9

# YOLOv8.0n head
head:
  - [-1, 1, nn.Upsample, [None, 2, 'nearest']]
  - [[-1, 6], 1, Concat, [1]]  # cat backbone P4
  - [-1, 3, C2f, [512]]  # 12

  - [-1, 1, nn.Upsample, [None, 2, 'nearest']]
  - [[-1, 4], 1, Concat, [1]]  # cat backbone P3
  - [-1, 3, C2f, [256]]  # 15 (P3/8-small)

  - [-1, 1, Conv, [256, 3, 2]]
  - [[-1, 12], 1, Concat, [1]]  # cat head P4
  - [-1, 3, C2f, [512]]  # 18 (P4/16-medium)

  - [-1, 1, Conv, [512, 3, 2]]
  - [[-1, 9], 1, Concat, [1]]  # cat head P5
  - [-1, 3, C2f, [1024]]  # 21 (P5/32-large)

  - [[15, 18, 21], 1, Detect, [nc]]  # Detect(P3, P4, P5)

### 配置cfg文件夹下面的default.yaml关于训练相关的配置，有一些需要自行修改

# Ultralytics YOLO 🚀, AGPL-3.0 许可
# 中度增强 COCO 训练的默认训练设置和超参数

task: detect # (str) YOLO 任务，即 detect（检测）、segment（分割）、classify（分类）、pose（姿态）
mode: train # (str) YOLO 模式，即 train（训练）、val（验证）、predict（预测）、export（导出）、track（跟踪）、benchmark（基准测试）

# 训练设置 -------------------------------------------------------------------------------------------------------
model: # (str, 可选) 模型文件路径，即 yolov8n.pt、yolov8n.yaml
data: C:\Users\Administrator\Desktop\yolov8\ultralytics-main\data.yaml # (str, 可选) 数据文件路径，即 coco8.yaml
epochs: 300 # (int) 训练的轮次
time: # (float, 可选) 训练的小时数，如果提供则覆盖 epochs
patience: 10 # (int) 提前停止训练的无明显改进轮次
batch: 5 # (int) 每批次的图像数量（-1 表示自动批量）
imgsz: 640 # (int | list) 输入图像大小，训练和验证模式为 int，预测和导出模式为 list[h,w]
save: True # (bool) 保存训练检查点和预测结果
save_period: -1 # (int) 每 x 轮保存检查点（< 1 时禁用）
cache: False # (bool) True/ram、disk 或 False。使用缓存进行数据加载
device:  0 # (int | str | list, 可选) 运行设备，即 cuda device=0 或 device=0,1,2,3 或 device=cpu
workers: 1 # (int) 数据加载的工作线程数（如果是 DDP 则为每个 RANK）
project: # (str, 可选) 项目名称
name: # (str, 可选) 实验名称，结果保存到 'project/name' 目录
exist_ok: False # (bool) 是否覆盖现有实验
pretrained: True # (bool | str) 是否使用预训练模型（bool）或加载权重的模型（str）
optimizer: auto # (str) 使用的优化器，选择=[SGD、Adam、Adamax、AdamW、NAdam、RAdam、RMSProp、auto]
verbose: True # (bool) 是否打印详细输出
seed: 0 # (int) 重现性的随机种子
deterministic: True # (bool) 是否启用确定性模式
single_cls: False # (bool) 将多类数据作为单类训练
rect: False # (bool) 矩形训练（如果模式为 'train'）或矩形验证（如果模式为 'val'）
cos_lr: False # (bool) 使用余弦学习率调度器
close_mosaic: 10 # (int) 最后几轮禁用马赛克增强（0 表示禁用）
resume: False # (bool) 从上一个检查点恢复训练
amp: True # (bool) 自动混合精度（AMP）训练，选择=[True, False]，True 运行 AMP 检查
fraction: 1.0 # (float) 用于训练的数据集比例（默认 1.0，即所有训练集图像）
profile: False # (bool) 在训练期间为记录器分析 ONNX 和 TensorRT 速度
freeze: None # (int | list, 可选) 冻结前 n 层，或在训练期间冻结的层索引列表
multi_scale: False # (bool) 是否在训练期间使用多尺度

# 分割
overlap_mask: True # (bool) 训练期间掩码应重叠（仅限分割训练）
mask_ratio: 4 # (int) 掩码下采样比例（仅限分割训练）

# 分类
dropout: 0.0 # (float) 使用 dropout 正则化（仅限分类训练）

# 验证/测试设置 ----------------------------------------------------------------------------------------------------
val: True # (bool) 在训练期间验证/测试
split: val # (str) 用于验证的数据集划分，即 'val'、'test' 或 'train'
save_json: False # (bool) 将结果保存为 JSON 文件
save_hybrid: False # (bool) 保存标签的混合版本（标签 + 额外预测）
conf: # (float, 可选) 检测的对象置信度阈值（默认 0.25 预测，0.001 验证）
iou: 0.7 # (float) 非极大值抑制（NMS）的交并比（IoU）阈值
max_det: 300 # (int) 每张图像的最大检测数量
half: False # (bool) 使用半精度（FP16）
dnn: False # (bool) 使用 OpenCV DNN 进行 ONNX 推理
plots: True # (bool) 在训练/验证期间保存图表和图像

# 预测设置 -----------------------------------------------------------------------------------------------------
source: # (str, 可选) 图像或视频的来源目录
vid_stride: 1 # (int) 视频帧率步幅
stream_buffer: False # (bool) 缓冲所有流帧（True）或返回最近的帧（False）
visualize: False # (bool) 可视化模型特征
augment: False # (bool) 对预测源应用图像增强
agnostic_nms: False # (bool) 类别无关的 NMS
classes: # (int | list[int], 可选) 按类别过滤结果，即 classes=0 或 classes=[0,2,3]
retina_masks: False # (bool) 使用高分辨率分割掩码
embed: # (list[int], 可选) 从给定层返回特征向量/嵌入

# 可视化设置 ---------------------------------------------------------------------------------------------------
show: False # (bool) 如果环境允许，显示预测的图像和视频
save_frames: False # (bool) 保存预测的单个视频帧
save_txt: False # (bool) 将结果保存为 .txt 文件
save_conf: False # (bool) 保存带有置信度分数的结果
save_crop: False # (bool) 保存带有结果的裁剪图像
show_labels: True # (bool) 显示预测标签，例如 'person'
show_conf: True # (bool) 显示预测置信度，例如 '0.99'
show_boxes: True # (bool) 显示预测框
line_width: # (int, 可选) 边框线宽。如果为 None，则缩放到图像大小。

# 导出设置 ------------------------------------------------------------------------------------------------------
format: torchscript # (str) 导出格式，选择见 https://docs.ultralytics.com/modes/export/#export-formats
keras: False # (bool) 使用 Keras
optimize: False # (bool) TorchScript: 优化移动端
int8: False # (bool) CoreML/TF INT8 量化
dynamic: False # (bool) ONNX/TF/TensorRT: 动态轴
simplify: False # (bool) ONNX: 使用 `onnxslim` 简化模型
opset: # (int, 可选) ONNX: opset 版本
workspace: 4 # (int) TensorRT: 工作区大小（GB）
nms: False # (bool) CoreML: 添加 NMS

# 超参数 ------------------------------------------------------------------------------------------------------
lr0: 0.01 # (float) 初始学习率（即 SGD=1E-2，Adam=1E-3）
lrf: 0.01 # (float) 最终学习率（lr0 * lrf）
momentum: 0.937 # (float) SGD 动量/Adam beta1
weight_decay: 0.0005 # (float) 优化器权重衰减 5e-4
warmup_epochs: 3.0 # (float) 预热轮次（可以是小数）
warmup_momentum: 0.8 # (float) 预热初始动量
warmup_bias_lr: 0.1 # (float) 预热初始偏置学习率
box: 7.5 # (float) 边框损失增益
cls: 0.5 # (float) 类别损失增益（与像素数成比例）
dfl: 1.5 # (float) DFL 损失增益
pose: 12.0 # (float) 姿态损失增益
kobj: 1.0 # (float) 关键点对象损失增益
label_smoothing: 0.0 # (float) 标签平滑（比例）
nbs: 64 # (int) 名义批量大小
hsv_h: 0.015 # (float) 图像 HSV 色调增强（比例）
hsv_s: 0.7 # (float) 图像 HSV 饱和度增强（比例）
hsv_v: 0.4 # (float) 图像 HSV 亮度增强（比例）
degrees: 0.0 # (float) 图像旋转（+/- 度数）
translate: 0.1 # (float) 图像平移（+/- 比例）
scale: 0.5 # (float) 图像缩放（+/- 增益）
shear: 0.0 # (float) 图像剪切（+/- 度数）
perspective: 0.0 # (float) 图像透视（+/- 比例），范围 0-0.001
flipud: 0.0 # (float) 图像上下翻转（概率）
fliplr: 0.5 # (float) 图像左右翻转（概率）
bgr: 0.0 # (float) 图像通道 BGR（概率）
mosaic: 1.0 # (float) 图像马赛克（概率）
mixup: 0.0 # (float) 图像混合（概率）
copy_paste: 0.0 # (float) 分割复制粘贴（概率）
auto_augment: randaugment # (str) 分类的自动增强策略（randaugment、autoaugment、augmix）
erasing: 0.4 # (float) 分类训练期间随机擦除的概率（0-0.9），0 表示无擦除，必须小于 1.0
crop_fraction: 1.0 # (float) 分类的图像裁剪比例（0.1-1），1.0 表示不裁剪，必须大于 0

# 自定义 config.yaml ---------------------------------------------------------------------------------------------------
cfg: # (str, 可选) 用于覆盖 defaults.yaml

# 跟踪器设置 ------------------------------------------------------------------------------------------------------
tracker: botsort.yaml # (str) 跟踪器类型，选择=[botsort.yaml, bytetrack.yaml]
save_dir: D:\files # (str) 保存目录

